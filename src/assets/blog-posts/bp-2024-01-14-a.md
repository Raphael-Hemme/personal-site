### Context
Some time between summer and fall of last year (2023), I had the idea to write a small Node.js script that would read out the markdown files in my blog and io-garden directories one by one and create an index of the words with references to all of their locations (the path to the source files and the corresponding line numbers). Finally the resulting index should be written to a json file under a specified path which I could then use as a simple search index on my personal site to offer not just the ability to retrieve relevant content by activating a tag of interest but to also make it fully searchable by just entering a word or a word fragment into a search field.

I approached this little side project as an interesting learning opportunity, well aware that it would be just a very simple solution to a complex problem addressed by experts in Math or Computer Science for decades. Yet, I liked the idea - not at least because it gave me the opportunity to re-engage with Node. Furthermore I intended to use this project as a point of entry into the domain of search and I intentionally ignored existing solutions and best practices in this first approach to actually make some of the same mistakes as generations of programmers before me because I believe that actually making some of those mistakes allows for a deeper understanding of the solutions they came up with. But of course in a later stage, I plan to study how search is usually implemented by the community today. This is not my go-to approach to learning about a new field btw and I wouldn't recommend it in many situations but I believe it can be worth a try in some. 
This intentional ignorance or naivitÃ© is, why I called my project: '(The) Naive Search Preprocessor' or 'TNSP' in short.

### Current state of the project
As you read this, the search on my site is powered by the current version of TNSP, enhanced with some frontend logic to manage search input and display results. I'm really happy about that and encourage you to give it a try.

The whole code including some preliminary idiosyncrasies can be found on [GitHub](https://github.com/Raphael-Hemme/naive-search-preprocessor). The code is not well documented yet but I'll work on that while writing a small series of posts about TNSP here. For now, I will just give a short overview of the current state of the project and how it works.

The current implementation of TNSP is a Node script that is run from the command line and generates the index as a local json file which can then be used in another place like my website as a search index. The script takes multiple arguments that specify the source paths and the output path + file name.

A full command to run the script could look like this: `node index.js --source ./dir-one ./dir-two --target ./index.json`.

The short form flags `-s` instead of `--source` and `-t` instead of `--target` do work as well and you can change the order of appearance.

If the user did not specify the source and target paths as arguments to the command, or if invalid paths are detected, the script will enter a CLI mode where the user is prompted to enter the missing or invalid paths.

I developed the script to be eventually run automatically in a pre-push hook or something similar where the arguments could be provided in advance in the call to the script. The CLI mode was initially just a fallback and also another opportunity to experiment with writing a Node CLI. But the CLI mode grew in scope and I enjoy working on it. Therefore I'm currently still adding small improvements or fix issues while executing the script manually. But the automation will be addressed and then also be documented in a blog post. 

### Future directions
Since the size of the output file (the search index json file) is currently about 1.1mb which in my opinion is a bit too large to send it over the wire, and since the generation of the index is quite fast (at least running the current implementation on my machine), I'm considering re-implementing it client-side. In that case the index could be generated on demand or pre-generated when everything else on the root path is loaded and then stored in memory (inside an Angular service or proper state management system like NGRX or NGXS). The downsides to this solution are 1. that the user's machine will be hit with a processing spike instead of a spike in network traffic and 2. that it necessitates to previously load all the markdown files. Currently the combined size of the markdown files (excluding this one) is just about 160kb - so considerably less than the search index (~1100kb). This would be a plus for the client-side generation approach at the moment. Should I magically increase my output in the future though - which I don't think is very likely -, this advantage would most likely turn into a disadvantage since I expect the size of the search index to increase logarithmically (? -> an initially steep curve that flattens out at some point) while the combined size of the markdown files should increase more or less linear with the addition of each file.
In short: I'm not sure which approach is better here but I think I'll experiment with the client-side solution at some point. Also, my current evaluation could be changed when studying existing search solutions or maybe there is a way to compress or encode the index in a more efficient way. In any case, I'm looking forward to more tinkering and learning with this project and hope you find some joy in reading it.

### How it works
... coming soon.


## Context
Some time between summer and fall of last year (2023), I had the idea to write a small Node script that would read out the markdown files in my blog and io-garden directories one by one and create an index of the words used. The individual words should then be assigned the path to the respective source files and the line numbers. Finally the resulting index object should be written to a json file on a specified path which I could then use as a simple search index on my personal site to offer not just the ability to retrieve relevant posts and io-garden experiments by activating a tag of interest but to also make them fully searchable by just entering a word or a word fragment into a search field.

I approached this little side project as an interesting learning opportunity - fully aware that it would be just a very simple solution to a problem that is addressed by people with Phd's in Math or Computer Science for decades and that my approach would most likely be technically anything but optimal. Yet, I liked it. Not at least because it gave me the opportunity to re-engage with Node. Furthermore I intended to use this project as a point of entry into search where I intentionally ignored existing solutions and best practices in a first step to actually make some of the mistakes that generations of programmers before me already made to better understand the solutions they came up with when, in a later stage, I would study how search was usually implemented by the community today. This is, why I called my project: '(The) Naive Search Preprocessor' or 'TNSP' in short.

## Current state of the project
As you read this, the current version of TNSP plus some frontend logic to handle search input and displaying the results is powering the search on my site and you can try it out which I am really happy about.

## Future directions
Since the size of the output file (the search index json file) is currently about 800kb which in my opinion is a bit to large to send it over the wire, and since the generation of the index is quite fast (at least running the current implementation on my machine), I'm considering re-implementing it client-side. In that case the index could be generated on demand or pre-generated when everything else on the root path is loaded and then stored in memory (inside an Angular service or proper state management system like NGRX or NGXS). The downside to this solution is that the user's machine will be hit with a processing spike instead of a spike in network traffic and that it necessitates to previously load all the markdown files. Currently the combined size of the markdown files (excluding this one) is just about 160kb - so considerably less than the search index (800kb). This would be a plus for the client-side generation approach at the moment. Should I magically increase my output in the future though - which I don't think is very likely -, this advantage would most likely turn into a disadvantage since I expect the size of the search index to increase logarithmically (? -> an initially steep curve that flattens out at some point) while the combined size of the markdown files should increase more or less linear with the addition of each file.
In short: I'm not sure which approach is better here but I think I'll give the client-side solution a try some time. Also, my current evaluation could be changed when studying existing search solutions or maybe there is a way to compress or encode the index in a more efficient way. In any case, I'm looking forward to more tinkering and learning with this project and hope you also get something from reading it.